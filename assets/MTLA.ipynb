{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVqSkzb6LXufuFVHFQMR4X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/D-Keqi/mtla/blob/main/assets/MTLA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-head Temporal Latent Attention (MTLA) Demo\n",
        "\n",
        "This notebook demonstrates:\n",
        "1. Training a simple language model using MTLA\n",
        "2. Implementing beam search decoding with incremental state\n",
        "3. Showing how the temporal compression works during inference\n",
        "\n",
        "[GitHub Project](https://github.com/D-Keqi/mtla)\n",
        "\n",
        "More specific usage examples of MTLA in Fairseq refer to [here](https://github.com/D-Keqi/mtla/blob/main/experiments/tools/fairseq/fairseq/models/transformer/transformer_decoder.py#L1638).\n"
      ],
      "metadata": {
        "id": "C9RPdpTgquBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "id": "9q2Z5h4wwZ_k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "713c4d39-b5a2-4350-aa1e-db55d6460bdf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x782e91501250>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Load the MTLA Module"
      ],
      "metadata": {
        "id": "dvM5tZj_wpd_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/D-Keqi/mtla.git\n",
        "%cd mtla"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwMLvoAIwuml",
        "outputId": "9c638499-1c2d-44ae-8f15-038850b83668"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mtla'...\n",
            "remote: Enumerating objects: 2313, done.\u001b[K\n",
            "remote: Counting objects: 100% (2313/2313), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1753/1753), done.\u001b[K\n",
            "remote: Total 2313 (delta 520), reused 2218 (delta 461), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (2313/2313), 13.11 MiB | 17.37 MiB/s, done.\n",
            "Resolving deltas: 100% (520/520), done.\n",
            "/content/mtla\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from MTLA import MultiheadTemporalLatentAttention"
      ],
      "metadata": {
        "id": "-huRILfqzL-b"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Create a Simple Language Model with MTLA"
      ],
      "metadata": {
        "id": "Y4ghbYR1UBk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiheadTemporalLatentAttention(\n",
        "            embed_dim=embed_dim,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            q_lora_rank=0,\n",
        "            kv_lora_rank=256,\n",
        "            qk_nope_head_dim=64,\n",
        "            qk_rope_head_dim=32,\n",
        "            v_head_dim=64,\n",
        "            down_rate=2,\n",
        "            recompute_prompt_attn=True,\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(embed_dim, 4 * embed_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * embed_dim, embed_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, position, incremental_state=None, self_attn_mask=None):\n",
        "        # Self attention\n",
        "        residual = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.self_attn(\n",
        "            query=x, #[:,-1:] if incremental_state is not None else x,\n",
        "            key=x,\n",
        "            value=x,\n",
        "            position=position,\n",
        "            incremental_state=incremental_state,\n",
        "            self_attn_mask=self_attn_mask\n",
        "        )\n",
        "        x = self.dropout(x)\n",
        "        x = residual + x\n",
        "\n",
        "        # Feed forward\n",
        "        residual = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ffn(x)\n",
        "        x = residual + x\n",
        "\n",
        "        return x\n",
        "\n",
        "class SimpleLM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=512, num_heads=8, num_layers=6):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, incremental_state=None, positions=None):\n",
        "        # Get positions if not provided\n",
        "        if positions is None:\n",
        "            positions = torch.arange(x.size(1), device=x.device).unsqueeze(0)\n",
        "\n",
        "        # Embedding and positional encoding\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # Create attention mask for causal (autoregressive) LM\n",
        "        seq_len = x.size(1)\n",
        "        self_attn_mask = torch.triu(\n",
        "            torch.full((seq_len, seq_len), float('-inf')), diagonal=1\n",
        "        ).to(x.device)\n",
        "\n",
        "        # Forward through layers\n",
        "        for layer in self.layers:\n",
        "            x = layer(\n",
        "                x,\n",
        "                position=positions,\n",
        "                incremental_state=incremental_state,\n",
        "                self_attn_mask=None, #self_attn_mask if incremental_state is None else None\n",
        "            )\n",
        "\n",
        "        x = self.norm(x)\n",
        "        logits = self.head(x)\n",
        "        return logits\n",
        "\n",
        "    def reorder_incremental_state(self, incremental_state, new_order):\n",
        "        \"\"\"Reorder incremental state for beam search\"\"\"\n",
        "        if incremental_state is None:\n",
        "            return\n",
        "\n",
        "        for layer in self.layers:\n",
        "            layer.self_attn.reorder_incremental_state(incremental_state, new_order)"
      ],
      "metadata": {
        "id": "4tz0sIDlUNip"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Create a Simple Dataset"
      ],
      "metadata": {
        "id": "2XP_yDZ9VO5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTextDataset(Dataset):\n",
        "    def __init__(self, texts, vocab, seq_length=64):\n",
        "        self.vocab = vocab\n",
        "        self.seq_length = seq_length\n",
        "        self.data = []\n",
        "\n",
        "        for text in texts:\n",
        "            # Tokenize (simple character-level tokenization)\n",
        "            tokens = [vocab.get(c, vocab['<unk>']) for c in text]\n",
        "            # Create sliding windows\n",
        "            for i in range(0, len(tokens) - seq_length, seq_length // 2):\n",
        "                chunk = tokens[i:i + seq_length]\n",
        "                if len(chunk) == seq_length:\n",
        "                    self.data.append(chunk)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.data[idx], dtype=torch.long)\n",
        "\n",
        "# Sample data\n",
        "texts = [\n",
        "    \"To be, or not to be: that is the question\",\n",
        "    \"All the world's a stage, and all the men and women merely players\",\n",
        "    \"Romeo, Romeo! Wherefore art thou Romeo?\",\n",
        "    \"What's in a name? That which we call a rose by any other name would smell as sweet\",\n",
        "    \"The lady doth protest too much, methinks\",\n",
        "    \"Brevity is the soul of wit\",\n",
        "    \"Uneasy lies the head that wears the crown\",\n",
        "    \"Parting is such sweet sorrow\",\n",
        "    \"Cowards die many times before their deaths\",\n",
        "    \"Some are born great, some achieve greatness\",\n",
        "    \"The course of true love never did run smooth\",\n",
        "    \"All that glitters is not gold\",\n",
        "    \"Love looks not with the eyes, but with the mind\",\n",
        "    \"Fair is foul, and foul is fair\",\n",
        "    \"The better part of valor is discretion\",\n",
        "    \"This above all: to thine own self be true\",\n",
        "    \"The fault, dear Brutus, is not in our stars, but in ourselves\",\n",
        "    \"How sharper than a serpent's tooth it is to have a thankless child\",\n",
        "    \"There are more things in heaven and earth, Horatio, than are dreamt of in your philosophy\",\n",
        "    \"What's done cannot be undone\"\n",
        "]\n",
        "\n",
        "# Create vocabulary\n",
        "chars = sorted(list(set(''.join(texts))))\n",
        "vocab = {c: i+2 for i, c in enumerate(chars)}\n",
        "vocab['<pad>'] = 0\n",
        "vocab['<unk>'] = 1\n",
        "vocab['<eos>'] = len(vocab)\n",
        "\n",
        "# Create dataset\n",
        "dataset = SimpleTextDataset(texts, vocab, seq_length=64)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "TjIVxrRLVPnw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Training Loop"
      ],
      "metadata": {
        "id": "fzwtyZ77VVQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = SimpleLM(vocab_size=len(vocab)).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=vocab['<pad>'])\n",
        "\n",
        "def train_epoch(model, dataloader):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Shift input and target\n",
        "        inputs = batch[:, :-1]\n",
        "        targets = batch[:, 1:]\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(inputs)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(logits.view(-1, logits.size(-1)), targets.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "# Train for a few epochs\n",
        "for epoch in range(10):\n",
        "    loss = train_epoch(model, dataloader)\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aW4pOdClVXlf",
        "outputId": "b97e56fe-9aa2-4708-eb12-f1b08e80899c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1/1 [00:01<00:00,  1.37s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 4.0091\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Loss: 3.0581\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1/1 [00:00<00:00,  1.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Loss: 2.6040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1/1 [00:01<00:00,  1.06s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Loss: 2.3303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1/1 [00:01<00:00,  1.37s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Loss: 2.1954\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1/1 [00:01<00:00,  1.10s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6, Loss: 2.0847\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7, Loss: 1.9640\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8, Loss: 1.8717\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1/1 [00:00<00:00,  1.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9, Loss: 1.8229\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Loss: 1.7630\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Beam Search Implementation\n",
        "This shows how to use the incremental state during decoding"
      ],
      "metadata": {
        "id": "M0ufWi9uVmcJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ParallelBeamSearchDecoder:\n",
        "    def __init__(self, model, beam_size=5, max_len=20):\n",
        "        self.model = model\n",
        "        self.beam_size = beam_size\n",
        "        self.max_len = max_len\n",
        "        self.vocab_size = model.head.out_features\n",
        "\n",
        "    def _init_incremental_state(self, batch_size=1):\n",
        "        \"\"\"Initialize incremental state similar to Fairseq's implementation\"\"\"\n",
        "        incremental_state = defaultdict(dict)\n",
        "        return incremental_state\n",
        "\n",
        "    def decode(self, initial_input):\n",
        "        \"\"\"Perform parallel beam search decoding with proper incremental state\"\"\"\n",
        "        # Initialize beams with proper incremental state\n",
        "        beams = {\n",
        "            'tokens': [initial_input[0].tolist()],  # List of token sequences\n",
        "            'scores': torch.zeros(1, device=initial_input.device),  # (num_beams,)\n",
        "            'parent_idx': None,  # For tracking beam origins\n",
        "            'incremental_state': self._init_incremental_state()  # Properly initialized\n",
        "        }\n",
        "\n",
        "        for step in range(self.max_len):\n",
        "            # Prepare input for all active beams\n",
        "            num_beams = len(beams['tokens'])\n",
        "\n",
        "            # Create input tensor: (num_beams, 1) - just the last token of each beam\n",
        "            if step == 0:\n",
        "              input_tensor = torch.tensor(\n",
        "                  [seq for seq in beams['tokens']],\n",
        "                  device=initial_input.device\n",
        "              )  # (num_beams, 1)\n",
        "            else:\n",
        "              input_tensor = torch.tensor(\n",
        "                  [seq[-1] for seq in beams['tokens']],\n",
        "                  device=initial_input.device\n",
        "              ).unsqueeze(1)  # (num_beams, 1)\n",
        "\n",
        "            # Create position tensor\n",
        "            if step == 0:\n",
        "              positions = (\n",
        "                  torch.arange(0, initial_input.size(1)+step, device=initial_input.device)\n",
        "                  .float()\n",
        "                  .view(1, -1)\n",
        "                )\n",
        "              positions = positions.repeat(num_beams, 1)\n",
        "            else:\n",
        "              positions = torch.tensor(\n",
        "                  [[len(seq)-1] for seq in beams['tokens']],\n",
        "                  device=initial_input.device,\n",
        "                  dtype=torch.float\n",
        "              )  # (num_beams, 1)\n",
        "\n",
        "            # Forward pass with incremental state\n",
        "            with torch.no_grad():\n",
        "                logits = self.model(\n",
        "                    input_tensor,\n",
        "                    incremental_state = beams['incremental_state'],\n",
        "                    positions = positions\n",
        "                )  # (num_beams, 1, vocab_size)\n",
        "\n",
        "            # Calculate scores for all possible continuations\n",
        "            log_probs = F.log_softmax(logits[:, -1, :], dim=-1)  # (num_beams, vocab_size)\n",
        "\n",
        "            # Combine with beam scores (broadcasting)\n",
        "            candidate_scores = beams['scores'].unsqueeze(1) + log_probs  # (num_beams, vocab_size)\n",
        "\n",
        "            # Flatten to get top candidates across all beams\n",
        "            flat_scores = candidate_scores.view(-1)\n",
        "            top_scores, top_indices = flat_scores.topk(self.beam_size)\n",
        "\n",
        "            # Determine which beam and token each top candidate comes from\n",
        "            beam_indices = top_indices // self.vocab_size\n",
        "            token_indices = top_indices % self.vocab_size\n",
        "\n",
        "            # Prepare new beams\n",
        "            new_tokens = []\n",
        "            new_scores = top_scores\n",
        "            new_parent_idx = beam_indices\n",
        "\n",
        "            # Build new token sequences\n",
        "            for i, beam_idx in enumerate(beam_indices):\n",
        "                new_seq = beams['tokens'][beam_idx] + [token_indices[i].item()]\n",
        "                new_tokens.append(new_seq)\n",
        "\n",
        "            # Reorder incremental state to match new beam order\n",
        "            self.model.reorder_incremental_state(beams['incremental_state'], beam_indices)\n",
        "\n",
        "            # Update beams for next iteration\n",
        "            beams = {\n",
        "                'tokens': new_tokens,\n",
        "                'scores': new_scores,\n",
        "                'parent_idx': new_parent_idx,\n",
        "                'incremental_state': beams['incremental_state']\n",
        "            }\n",
        "\n",
        "            # Early stopping if all beams end with EOS (not implemented here)\n",
        "            # For demo we'll just use max_len\n",
        "\n",
        "        # Return best sequence (normalized by length)\n",
        "        best_idx = torch.argmax(beams['scores'] / torch.tensor(\n",
        "            [len(seq) for seq in beams['tokens']],\n",
        "            device=beams['scores'].device\n",
        "        ))\n",
        "        return beams['tokens'][best_idx]\n",
        "\n",
        "# Initialize decoder with parallel implementation\n",
        "model.eval()\n",
        "parallel_decoder = ParallelBeamSearchDecoder(model, beam_size=5, max_len=20)\n",
        "\n",
        "# Create a test input\n",
        "vocab_size = len(vocab)\n",
        "inv_vocab = {v: k for k, v in vocab.items()}  # Inverse vocabulary mapping\n",
        "\n",
        "# Initialize with prompt tokens\n",
        "prompt=\"The quick brown\"\n",
        "initial_tokens = [vocab.get(c, vocab['<unk>']) for c in prompt]\n",
        "test_input = torch.tensor([initial_tokens], device=device)\n",
        "\n",
        "# Decode with parallel beam search\n",
        "decoded_seq = parallel_decoder.decode(test_input)\n",
        "test_out = ''.join([inv_vocab.get(t, '<unk>') for t in decoded_seq])\n",
        "print(\"Decoded sequence:\", test_out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yagxuljpVp3l",
        "outputId": "1bef09c2-e6bc-4ae3-f80c-a6ea02e0d9d5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoded sequence: The quick brownd and and and thand \n"
          ]
        }
      ]
    }
  ]
}